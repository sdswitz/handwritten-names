{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Handwritten Names Recognition - Google Colab Training\n\nThis notebook trains a Transformer model for handwritten name recognition using Google Colab's GPU.\n\n**Before running:**\n1. Get your Kaggle API key: Go to kaggle.com → Account → Create New API Token\n2. Set runtime to GPU: `Runtime > Change runtime type > T4 GPU`\n3. Run cells in order\n\n**Dataset will be downloaded directly from Kaggle - no need to upload!**\n\n**Model:** Using Vision Transformer (ViT) with patch embeddings (~5.8M parameters). To switch to CRNN, change `USE_TRANSFORMER = False` in the config cell below."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (only for saving checkpoints)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/sdswitz/handwritten-names.git\n",
    "%cd handwritten-names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision pillow pandas numpy tqdm python-Levenshtein kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset from Kaggle\n",
    "\n",
    "Upload your `kaggle.json` file when prompted below.\n",
    "\n",
    "**To get kaggle.json:**\n",
    "1. Go to https://www.kaggle.com/\n",
    "2. Click your profile picture → Account\n",
    "3. Scroll to API section\n",
    "4. Click \"Create New API Token\"\n",
    "5. Upload the downloaded file below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload kaggle.json\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Setup Kaggle credentials\n",
    "!mkdir -p ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "print(\"✓ Kaggle credentials configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from Kaggle\n",
    "print(\"Downloading dataset from Kaggle...\")\n",
    "!kaggle datasets download -d landlord/handwriting-recognition\n",
    "\n",
    "print(\"\\nExtracting dataset...\")\n",
    "!unzip -q handwriting-recognition.zip -d /content/data\n",
    "\n",
    "print(\"\\n✓ Dataset ready!\")\n",
    "!ls -lh /content/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure for Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Overwrite config.py with Colab-specific settings\nconfig_updates = \"\"\"import torch\n\nclass Config:\n    # Paths - data downloaded from Kaggle to /content/data/\n    DATA_DIR = '/content/data/'\n    TRAIN_CSV = 'written_name_train_v2.csv'\n    VAL_CSV = 'written_name_validation_v2.csv'\n    TEST_CSV = 'written_name_test_v2.csv'\n    TRAIN_IMG_DIR = 'train_v2/train'\n    VAL_IMG_DIR = 'validation_v2/validation'\n    TEST_IMG_DIR = 'test_v2/test'\n\n    # Model checkpoints - saved to Google Drive\n    CHECKPOINT_DIR = '/content/drive/MyDrive/handwritten-names/checkpoints'\n\n    # Image settings\n    IMG_HEIGHT = 128\n    IMG_WIDTH = 512\n    NUM_CHANNELS = 1\n\n    # Character vocabulary\n    CHARS = ' ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n    BLANK_LABEL = len(CHARS)\n    NUM_CLASSES = len(CHARS) + 1\n\n    # Model selection\n    USE_TRANSFORMER = True  # Toggle between CRNN and Transformer\n\n    # CRNN Model architecture (not in use - fallback option)\n    # Uncomment these if USE_TRANSFORMER = False\n    # CNN_OUTPUT_CHANNELS = 512\n    # RNN_HIDDEN_SIZE = 256\n    # RNN_NUM_LAYERS = 2\n    # RNN_DROPOUT = 0.2\n\n    # Transformer architecture settings (active by default)\n    PATCH_SIZE = 64  # Size of each patch (64x64)\n    EMBED_DIM = 256  # Embedding dimension\n    TRANSFORMER_LAYERS = 6  # Number of encoder layers\n    TRANSFORMER_HEADS = 8   # Number of attention heads\n    TRANSFORMER_DIM_FF = 1024  # Feed-forward dimension\n    TRANSFORMER_DROPOUT = 0.1\n\n    # Training hyperparameters\n    BATCH_SIZE = 32\n    NUM_EPOCHS = 50\n    LEARNING_RATE = 0.001\n    WEIGHT_DECAY = 1e-5\n\n    # Device - will use GPU on Colab\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # DataLoader\n    NUM_WORKERS = 2\n    PIN_MEMORY = True\n\n    # Early stopping\n    PATIENCE = 5\n\n    # Logging\n    PRINT_FREQ = 100\n    SAVE_FREQ = 5  # Save every 5 epochs\n\"\"\"\n\nwith open('config.py', 'w') as f:\n    f.write(config_updates)\n\nprint(\"✓ Config updated for Colab\")\nprint(f\"✓ Using {'Transformer' if 'USE_TRANSFORMER = True' in config_updates else 'CRNN'} architecture\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU is available\n",
    "import torch\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ WARNING: No GPU detected! Training will be very slow.\")\n",
    "    print(\"Go to Runtime > Change runtime type > Select GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Training\n\nThis will train the model and save checkpoints to your Google Drive.\n- **Model:** Vision Transformer with patch embeddings (~5.8M parameters)\n- **Dataset:** Downloaded from Kaggle (in Colab's temp storage)\n- **Checkpoints:** Saved to `MyDrive/handwritten-names/checkpoints/best_model.pth`\n- **Training time:** 1-3 hours depending on GPU (Transformer is faster than CRNN due to parallelization)\n- You can close your browser - training continues in background\n\n**Note:** To switch to CRNN architecture (~15M parameters), change `USE_TRANSFORMER = True` to `False` in the config cell above and re-run it before training."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (Optional)\n",
    "\n",
    "After training completes, evaluate the model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "!python evaluate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Single Image (Optional)\n",
    "\n",
    "Test the model on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single image\n",
    "!python inference.py --image /content/data/test_v2/test/TEST_00001.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model Weights (Optional)\n",
    "\n",
    "Your model is already saved in Google Drive, but you can also download it directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download best model to your local machine\n",
    "from google.colab import files\n",
    "files.download('/content/drive/MyDrive/handwritten-names/checkpoints/best_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Training History (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = pd.read_csv('/content/drive/MyDrive/handwritten-names/checkpoints/training_history.csv')\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss')\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# CER\n",
    "axes[0, 1].plot(history['train_cer'], label='Train CER')\n",
    "axes[0, 1].plot(history['val_cer'], label='Val CER')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('CER')\n",
    "axes[0, 1].set_title('Character Error Rate')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# WER\n",
    "axes[1, 0].plot(history['train_wer'], label='Train WER')\n",
    "axes[1, 0].plot(history['val_wer'], label='Val WER')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('WER')\n",
    "axes[1, 0].set_title('Word Error Rate')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[1, 1].plot(history['train_acc'], label='Train Accuracy')\n",
    "axes[1, 1].plot(history['val_acc'], label='Val Accuracy')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].set_title('Accuracy')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best validation CER: {history['val_cer'].min():.4f}\")\n",
    "print(f\"Best validation accuracy: {history['val_acc'].max():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}